{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Prevent memory hogging\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12685c",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f83e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline utils\n",
    "error_images = []\n",
    "@tf.py_function(Tout=(tf.string, tf.float32))\n",
    "def load_image(file_path):\n",
    "    try:\n",
    "        img = Image.open(file_path.numpy().decode()).convert('RGB')\n",
    "        img = img.resize((256, 256))\n",
    "        img = tf.convert_to_tensor(np.array(img), dtype=tf.float32) / 255.0\n",
    "        return file_path, img\n",
    "    except Exception as e:\n",
    "        # Return dummy tensor and mark with empty string path\n",
    "        error_images.append(file_path.numpy().decode())\n",
    "        return tf.constant('', dtype=tf.string), tf.zeros([256, 256, 3], dtype=tf.float32)\n",
    "\n",
    "def filter_invalid(file_path, image):\n",
    "    return tf.strings.length(file_path) > 0\n",
    "\n",
    "def build_dataset(image_paths, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=4)\n",
    "    dataset = dataset.filter(filter_invalid)\n",
    "    dataset = dataset.batch(batch_size).prefetch(4)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9203e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "def load_model(ckpt):\n",
    "    try:\n",
    "        model = keras.models.load_model(ckpt)\n",
    "        print(f\"Model loaded successfully from {ckpt}\")\n",
    "        # model.summary()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {ckpt}: {e}\")\n",
    "        return None\n",
    "\n",
    "def infer(model, dataset):\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataset):\n",
    "        paths, imgs = batch # Unpack paths and images\n",
    "        # Inference\n",
    "        preds = model.predict(imgs, verbose=0)\n",
    "        # Process predictions\n",
    "        for i in range(len(paths)):\n",
    "            haze_color = np.argmax(preds[0][i])\n",
    "            haze_density = np.argmax(preds[1][i])\n",
    "            predictions.append({\n",
    "                'image_path': paths[i].numpy().decode(),\n",
    "                'haze_color': haze_color,\n",
    "                'haze_density': haze_density,\n",
    "            })\n",
    "        del paths, imgs, preds # Clear memory\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441a2b0",
   "metadata": {},
   "source": [
    "# Visualize color clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a065ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_CLUSTERS = {\n",
    "    1: [\n",
    "        (246.48, 254.53),\n",
    "        (213.99, 251.55),\n",
    "        (149.79, 239.74)\n",
    "    ],\n",
    "    2: [\n",
    "        (146.16, 217.47),\n",
    "        (181.47, 230.52),\n",
    "        (205.76, 241.87)\n",
    "    ],\n",
    "    3: [\n",
    "        (218.99, 255.0),\n",
    "        (217.01, 255.0),\n",
    "        (215.25, 255.0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize densities\n",
    "HAZE_DENSITIES = {\n",
    "    0: (0.01, 0.075), # Light\n",
    "    1: (0.075, 0.25),  # Medium\n",
    "    2: (0.25, 0.5)    # Heavy\n",
    "}\n",
    "\n",
    "def viz_cluster_color_range(cluster_id='0', step=5):\n",
    "    cluster = COLOR_CLUSTERS[cluster_id] # Get cluster\n",
    "    # Get value range for each color channel\n",
    "    red_range = np.linspace(cluster[0][0], cluster[0][1], step)\n",
    "    green_range = np.linspace(cluster[1][0], cluster[1][1], step)\n",
    "    blue_range = np.linspace(cluster[2][0], cluster[2][1], step)\n",
    "    # Get color list\n",
    "    colors = []\n",
    "    for i in range(step):\n",
    "        colors.append((\n",
    "            int(red_range[i]),\n",
    "            int(green_range[i]),\n",
    "            int(blue_range[i])\n",
    "        ))\n",
    "    f, ax = plt.subplots(1, step, figsize=(10,10), tight_layout=True)\n",
    "    for i in range(step):\n",
    "        color_arr = np.ones((100, 100, 3), dtype=np.uint8) * np.asarray(colors[i])\n",
    "        ax[i].imshow(color_arr)\n",
    "        ax[i].axis('off')\n",
    "        ax[i].set_title(str(colors[i]))\n",
    "\n",
    "# Visualize color ranges for each cluster\n",
    "for key in COLOR_CLUSTERS.keys():\n",
    "    print(f'Cluster {key}: {COLOR_CLUSTERS[key]}')\n",
    "    viz_cluster_color_range(key, step=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b193d0c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24918ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN \n",
    "image_paths = glob.glob('/home/duynd/haze_attrs/crawled_images/*/*.*') # Change to your image directory\n",
    "print(f\"Found {len(image_paths)} images.\")\n",
    "# Load the pre-trained model\n",
    "model = load_model('haze_classifier_tuned_v1.keras')\n",
    "# Load inference dataset\n",
    "dataset = build_dataset(image_paths, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file\n",
    "output_file = 'haze_attributes.csv' # Output CSV file for predictions\n",
    "error_file = 'error_images.txt' # Error log file for images that failed to load\n",
    "\n",
    "# Perform inference\n",
    "predictions = infer(model, dataset)\n",
    "# Save error images\n",
    "if len(error_images) > 0:\n",
    "    with open(error_file, 'w') as f:\n",
    "        for img in error_images:\n",
    "            f.write(f\"{img}\\n\")\n",
    "    print(f\"Error images saved to error_images.txt. Total: {len(error_images)}\")\n",
    "# Save predictions as csv\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acac03",
   "metadata": {},
   "source": [
    "# OPTIONAL: Sort images by colors and desities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321694e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# OPTIONAL: Save results to attribute directories\n",
    "save_dir = './results'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "new_predictions = []\n",
    "\n",
    "# Create directories for each haze color and density\n",
    "for color in COLOR_CLUSTERS.keys():\n",
    "    color_dir = os.path.join(save_dir, 'color', f'color_{color}')\n",
    "    if not os.path.exists(color_dir):\n",
    "        os.makedirs(color_dir)\n",
    "for density in HAZE_DENSITIES.keys():\n",
    "    density_dir = os.path.join(save_dir, 'density', f'density_{density}')\n",
    "    if not os.path.exists(density_dir):\n",
    "        os.makedirs(density_dir)\n",
    "\n",
    "index = 0\n",
    "for prediction in tqdm(predictions):\n",
    "    image_path = prediction['image_path']\n",
    "    haze_color = prediction['haze_color']\n",
    "    haze_density = prediction['haze_density']\n",
    "    \n",
    "    # Copy the image to the appropriate directory\n",
    "    color_dest_path = os.path.join(save_dir, 'color', f'color_{haze_color}', f'{index:06d}.jpg')\n",
    "    os.makedirs(os.path.dirname(color_dest_path), exist_ok=True)\n",
    "    shutil.copy(image_path, color_dest_path)\n",
    "    \n",
    "    density_dest_path = os.path.join(save_dir, 'density', f'density_{haze_density}', f'{index:06d}.jpg')\n",
    "    os.makedirs(os.path.dirname(density_dest_path), exist_ok=True)\n",
    "    shutil.copy(image_path, density_dest_path)\n",
    "    index += 1\n",
    "\n",
    "    # Append to new predictions list\n",
    "    new_predictions.append({\n",
    "        'image_path': image_path,\n",
    "        'haze_color': haze_color,\n",
    "        'haze_density': haze_density,\n",
    "        'color_dest_path': color_dest_path,\n",
    "        'density_dest_path': density_dest_path\n",
    "    })\n",
    "\n",
    "# Save new predictions to CSV\n",
    "new_predictions_df = pd.DataFrame(new_predictions)\n",
    "new_predictions_df.to_csv(os.path.join(save_dir, 'haze_attributes_sorted.csv'), index=False)\n",
    "print(f\"Results saved to {save_dir} with sorted images by haze color and density.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
